{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee3ca5f",
   "metadata": {},
   "source": [
    "### Nearest Neighbor Classifier\n",
    "This classifier has nothing to do with CNNs and it's very rare to use in practice, but it will make to get an idea about the basic approach to an image classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0a00a",
   "metadata": {},
   "source": [
    "##### CIFAR-10\n",
    "The dataset consists of 60k tiny images that are 32 px high and wide.\n",
    "Each image is labeled with one of 10 classes. These 60k images are partitioned into a training set of 50k images and test set of 10k images.\n",
    "- Classes:\n",
    "    - airplane\n",
    "    - automobile\n",
    "    - bird\n",
    "    - cat\n",
    "    - deer\n",
    "    - dog\n",
    "    - frog\n",
    "    - horse\n",
    "    - ship\n",
    "    - truck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be0606",
   "metadata": {},
   "source": [
    "##### The Core Idea of kNN\n",
    "- Training: memorizing all the training images and their labels. There's no complex \"learning\" phase\n",
    "- Prediction: To classify a new test image, it compares it to every single training image to find 'k' most similar ones. It then holds a \"vote\" among these 'k' neighbors, and the most common label wins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420aef55",
   "metadata": {},
   "source": [
    "-> One of the simplest possibilities is to compare the images px-by-px and add up all the difference. In other words, given to images and representing them as vectors I1,I2 a reasonable choice for comparing them might be the L1 distance:\n",
    "$$\n",
    "d_1(I_1, I_2) = \\sum_p \\left| I_1^p - I_2^p \\right|\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ccb40",
   "metadata": {},
   "source": [
    "Step-by-step implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "165fdab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import pickle as pkl\n",
    "\n",
    "# downloading CIFAR-10 training data\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "\n",
    "# downloading CIFAR-10 test data\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "\n",
    "# extracting the data and labels into numpy arrays\n",
    "X_train, y_train = train_data.data, np.array(train_data.targets)\n",
    "X_test, y_test = test_data.data, np.array(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01265302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000,), (10000, 32, 32, 3), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50a052",
   "metadata": {},
   "source": [
    "the kNN classifier works with \"flattened\" images, meaning each 32x32x3 images should be turned into one vector of 3072 numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a5214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0], 32*32*3) # reshape(..., -1) is used too if data shape is unknown\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], 32*32*3) # reshape(..., -1) is used too if data shape is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd6dc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (10000, 3072))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_flat.shape, X_test_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca9162",
   "metadata": {},
   "source": [
    "Following code is the basic implementation of Nearest Neighbor classifier using L2 distance:\n",
    "$$\n",
    "d_2(I_1, I_2) = \\sqrt{\\sum_p (I_{p1} - I_{p2})^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9340011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighbor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        # classifier learns / memorizes the training data\n",
    "        self.Xtr = X # \n",
    "        self.ytr = y\n",
    "\n",
    "    def predict(self, X, k=1):\n",
    "        num_test = X.shape[0] # #test images to classify\n",
    "        Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # creating array to store predicted y values\n",
    "\n",
    "        # looping over all test rows\n",
    "        for i in range(num_test):\n",
    "            # distances = np.sum(np.abs(self.Xtr - X[i, :]), axis=1) \n",
    "            # X[i, :] selects the ith test image\n",
    "            \n",
    "            # L2 distance for accuracy purposes\n",
    "            distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))\n",
    "            \n",
    "            if k>1:\n",
    "                closest_indices = np.argsort(distances)[:k] # indices of the 'k' mallest distances\n",
    "                closest_labels = self.ytr[closest_indices]\n",
    "                most_label = np.argmax(np.bincount(closest_labels))\n",
    "                Ypred[i] = most_label\n",
    "            # if k=1\n",
    "            else:\n",
    "                min_index = np.argmin(distances) # index of smallest distance \n",
    "                Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n",
    "\n",
    "        return Ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c02a3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbor()\n",
    "nn.train(X_train_flat, y_train) # training classifier on train imgs and labels\n",
    "y_test_predict = nn.predict(X_test_flat[:1000], k=1) # [:1000] is for using smaller set to run fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6cbe704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.262)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test_predict == y_test[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891e2eb",
   "metadata": {},
   "source": [
    "'accuracy' only achieved 26.9%, which is pretty bad.\n",
    "but we know that it doesn't randomly guess between classes (it would be 10% since there is 10 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb80e53",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning for Validation sets\n",
    "kNN requires a set for k and for finding the best hyperparameters for model, such as the value of 'k' or the distances we used like L1 and L2 we shdn't use test set. Using test set would lead model overfitting.\n",
    "- Overfitting: model performs well on train data but bad on test data\n",
    "Since test set can't be used, we should split training data into two parts:\n",
    "- train set\n",
    "- validation set (fake test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11907a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ff05d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, accuracy: 24.30%\n",
      "k = 3, accuracy: 22.20%\n",
      "k = 5, accuracy: 21.70%\n",
      "k = 10, accuracy: 21.60%\n",
      "k = 20, accuracy: 21.60%\n",
      "k = 50, accuracy: 21.90%\n",
      "k = 100, accuracy: 23.10%\n"
     ]
    }
   ],
   "source": [
    "X_val_flat = X_train_flat[:1000, :] # take first 1000 for val set\n",
    "y_val = y_train[:1000]\n",
    "\n",
    "X_train_flat = X_train_flat[1000:, :] # take last 49k for train set\n",
    "y_train = y_train[1000:]\n",
    "\n",
    "validation_accuracies = {} # to keep hyperparameters work well\n",
    "\n",
    "nn = NearestNeighbor()\n",
    "nn.train(X_train_flat, y_train)\n",
    "\n",
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "\n",
    "    y_val_predict = nn.predict(X_val_flat, k=k)\n",
    "    \n",
    "    acc = np.mean(y_val_predict == y_val)\n",
    "\n",
    "    print(f'k = {k}, accuracy: {acc*100:.2f}%')\n",
    "\n",
    "    validation_accuracies[k] = acc # add k value and its accuracy to the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba99b6",
   "metadata": {},
   "source": [
    "We can now see which value of 'k' works best for model and now can stick with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b2f1e",
   "metadata": {},
   "source": [
    "There is also another way for hyperparameter tuning called 'cross validation'.\n",
    "- Cross Validation: instead of randomly picking the first 1000 datapoints to be the val set, we can get better estimate how well a certain value of k works by iterating over different validation sets and averaging the performance across these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf248a",
   "metadata": {},
   "source": [
    "so far, we implemented a kNN classifier from scratch to establish a baseline and undeerstand core ML concepts like,\n",
    "- Data-driven paradigm; collect data, let algo. learn patterns from examples, evaluate on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324edba",
   "metadata": {},
   "source": [
    "kNN achieves low-accuracy simply because\n",
    "- pixel-wise distances don't capture semantic similarity\n",
    "- high computational cost at test time\n",
    "- doesn't work well with high-D data\n",
    "- that's why there are concepts like CNNs which is more complex concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665586a",
   "metadata": {},
   "source": [
    "\"Summary directly from cs231n lecture notes itself\"\n",
    "- In summary:\n",
    "\n",
    "    - We introduced the problem of Image Classification, in which we are given a set of images that are all labeled with a single category. We are then asked to predict these categories for a novel set of test images and measure the accuracy of the predictions.\n",
    "    - We introduced a simple classifier called the Nearest Neighbor classifier. We saw that there are multiple hyper-parameters (such as value of k, or the type of distance used to compare examples) that are associated with this classifier and that there was no obvious way of choosing them.\n",
    "    - We saw that the correct way to set these hyperparameters is to split your training data into two: a training set and a fake test set, which we call validation set. We try different hyperparameter values and keep the values that lead to the best performance on the validation set.\n",
    "    - If the lack of training data is a concern, we discussed a procedure called cross-validation, which can help reduce noise in estimating which hyperparameters work best.\n",
    "    - Once the best hyperparameters are found, we fix them and perform a single evaluation on the actual test set.\n",
    "    - We saw that Nearest Neighbor can get us about 30% accuracy on CIFAR-10. It is simple to implement but requires us to store the entire training set and it is expensive to evaluate on a test image.\n",
    "    - Finally, we saw that the use of L1 or L2 distances on raw pixel values is not adequate since the distances correlate more strongly with backgrounds and color distributions of images than with their semantic content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
